\section{Demonstration of Improved Decision-Making}

For our model, we tried 4 different reward methods: mean of like, sum of like, mean of watch ratio, sum of watch ratio. These four different methodologies to quantify reward were chosen to compare how varying reward definition can influence the model's recommendation performance. 

\subsection{Mean of Like}


The mean of like model defines the reward of each arm as the mean of likes per video category. This model did not converge during training, as evidenced by the linear cumulative regret. In testing, the model produces a pseudo-random distribution of video category predictions \ref{mean_like}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{summary_report/images/mean_like.png}
    \caption{Model using mean of likes as grouping function per video category}
    \label{mean_like}
\end{figure}

\subsection{Sum of Like}

The sum of like model defines the reward of each arm as the sum of likes per video category. This model did converge during training, as evidenced by the flatting cumulative regret curve. However, the model generalizes poorly and is heavily biased towards one arm. While the bias is heavy, the model is not completely biased and sometimes pulls non-favorite arms. This is likely due to bias in the input data, and the heavy skew of the non-normalized sum function \ref{sum_like}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{summary_report/images/sum_like.png}
    \caption{Model using sum of likes as grouping function per video category}
    \label{sum_like}
\end{figure}

\subsection{Mean of Watch Ratio}

The mean of watch ratio model defines the reward of each arm as the mean of watch ratio per video category \ref{mean_watchratio}. This model did not converge during training, as evidenced by the linear cumulative regret. In testing, the model produces a pseudo-random distribution of video category predictions, similar to the sum of like model.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{summary_report/images/mean_watchratio.png}
    \caption{Model using mean of watch ratio as grouping function per video category}
    \label{mean_watchratio}
\end{figure}

\subsection{Sum of Watch Ratio}

The sum of watch ratio model defines the reward of each arm as the sum of watch ratio per video category. This model did converge during training, as evidenced by the flatting cumulative regret curve. However, the model generalizes poorly and is \textit{entirely} biased towards one arm. While the sum of like model sometimes pulls non-favorite arms, this model achieves 0 regret only pulling one arm, meaning there is one video category who's sum of watch ratio is higher for every user in the dataset \ref{sum_watchratio}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{summary_report/images/sum_watchratio.png}
    \caption{Model using sum of watch ratio as grouping function per video category}
    \label{sum_watchratio}
\end{figure}

\subsection{Models Comparisons}

Results indicate that video categories grouped by sum cause the model to converge but is biased toward one arm. In contrast, categories grouped by mean are not easily modeled by LinUCB, leading to failure to converge. In terms of regret minimization, the sum of watch ratio model performs the best. However, in terms of usefulness, the sum of likes model generalizes better than the sum of watch ratio. While both models are biased, the sum of likes model sometimes pulls arms which are not the clear favorite. The sum of watch ratio model only predicts one video category which is not helpful in practice. Additionally, the models grouped by mean are unhelpful since they fail to converge during training. This indicates poor generalization, and testing yields a pseudo-random distribution of predictions across categories. Another observation is that during testing the sum of watch ratio is flat at 0, and the arms pulled is biased for arm 28. It can be concluded, that the most helpful predictor is the sum of likes reward model. Although it is biased, it is able to get to converge better than the other models in testing.

\begin{table}[H]
\centering
\caption{Model Comparison Summary}
\renewcommand{\arraystretch}{1.5} % Adjust row height for readability
\setlength{\tabcolsep}{5.5pt} % Adjust column spacing
\begin{tabular}{|p{3.5cm}|c|c|c|c|}
\hline
\textbf{Category} & \textbf{Mean of Likes} & \textbf{Sum of Likes} & \textbf{Mean of Watch Ratio} & \textbf{Sum of Watch Ratio} \\ \hline
Convergence {\tiny (Yes/No)} & No & Yes & No & Yes \\ \hline
Bias {\tiny (Low/High/Very High)} & Low & High & Low & Very High \\ \hline
Generalization {\tiny (Poor/Moderate/Good)} & Moderate & Poor & Moderate & Poor \\ \hline
\end{tabular}
\label{tab:reward-methodology}
\end{table}



% We demonstrate LinUCB's effectiveness by simulating a sequence of recommendations on the prepared KuaiRec dataset.

% \subsection{Iterative Training}

% We iteratively present users from the training set to the bandit model (Figure \ref{regret_plot}). Each user's context and available arms (categories) are fed into LinUCB. The model selects a category to recommend, observes the resulting reward (watch ratio), and updates its parameters accordingly.
    
% \subsection{Cumulative Regret Analysis}

% We measure the cumulative regret of the recommendations, defined as the difference between the reward of the chosen arm and the best possible arm in hindsight. As the model learns, the cumulative regret curve flattens, indicating that LinUCB is converging toward optimal decisions. Reduced regret over time demonstrates improved decision-making and more effective recommendations.

% \subsection{Testing and Generalization}

% After training, we apply the learned model to the test set without further parameter updates (Figure \ref{regret_plot}. By comparing cumulative regret and average rewards between the training and testing phases, we ensure that the model's performance gains are not limited to the training sample. We observe that LinUCB converges but does not generalize well. Arms 25-27 are pulled 

% The result is a recommendation strategy that adaptively learns user preferences and adjusts its recommendations to maximize engagement, all while efficiently exploring new categories that might interest users.