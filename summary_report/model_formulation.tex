\section{Model Formulation and Estimation}

\subsection{Contextual Multi-Armed Bandit Framework}

The contextual MAB setting aims to dynamically make recommendation to users with the objective of maximizing engagement. In our implementation, the context are features for each users that capture information ranging from the categories of the video they watched, the time for how long the video was watched, to the date of when it was watched and the video duration. The objective of the model is to predict the first-level category that a user is most likely to engage with. Based on that recommendation, we are then able to calculate the reward. The goal is to select arms (categories) over a sequence of trials to maximize cumulative rewards while learning from evolving user contexts.

\subsection{LinUCB Algorithm}

To efficiently incorporate contextual information, we employ the LinUCB algorithm. LinUCB assumes that the expected reward of each arm is a linear function of the contextual features:

\[
r_{a}(x) = \theta_a^\top x + \epsilon,
\]

where \(r_{a}(x)\) is the reward for arm \(a\) given context \(x\), \(\theta_a\) is a parameter vector associated with arm \(a\), and \(\epsilon\) is noise. The algorithm maintains an estimate of \(\theta_a\) for each arm and uses an upper confidence bound strategy to balance exploration and exploitation.

For each arm \(a\), LinUCB maintains a matrix \(A_a\) and vector \(b_a\):

\[
A_a = I + \sum_{t: a_t = a} x_t x_t^\top, \quad b_a = \sum_{t: a_t = a} r_{t} x_t,
\]

where \(x_t\) is the context vector at time \(t\), and \(r_t\) is the observed reward. The estimate of \(\theta_a\) is:

\[
\hat{\theta}_a = A_a^{-1} b_a.
\]

When selecting an arm at time \(t\), LinUCB computes an upper confidence bound:

\[
\text{UCB}_a(x_t) = \hat{\theta}_a^\top x_t + \alpha \sqrt{x_t^\top A_a^{-1} x_t},
\]

where \(\alpha\) controls the degree of exploration. The algorithm selects the arm with the highest UCB value. Over time, this approach refines its parameter estimates and improves its ability to recommend categories that yield higher rewards.